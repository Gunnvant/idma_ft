{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7b3551-c098-49e9-b9a5-60cecc5b55b3",
   "metadata": {},
   "source": [
    "**Backpropagation Algorithm**\n",
    "\n",
    "This notebook illustrates the backprop algorithm for a relatively simple dataset. The network architecture is as follows:\n",
    "\n",
    "1. $z_1 = w_1x$\n",
    "2. $h_1 = tanh(z_1)$\n",
    "3. $z_2 = w_2h_1+b$\n",
    "4. $h_2 =tanh(z_2)$\n",
    "5. $C = \\frac{1}{2}(y-h_2)^2$\n",
    "\n",
    "Now the gradients needed would be for the following parameters:\n",
    "\n",
    "1. $w_1$\n",
    "2. $w_2$\n",
    "3. $b$\n",
    "\n",
    "Expressions for gradients of weight terms:\n",
    "\n",
    "$\\frac{\\partial(C)}{\\partial(w_1)} = -((1-tanh^2(z_1))x_1w_2(1-tanh^2(z_2))(y-h_2))$\n",
    "\n",
    "$\\frac{\\partial(C)}{\\partial(w_2)} = -((1-tanh^2(z_2))(y-h_2)h_1)$\n",
    "\n",
    "$\\frac{\\partial(C)}{\\partial(b)} = -((1-tanh^2(z_2))(y-h_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e035e08-63bc-4b74-9e37-c5f1efc5f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c7986e-de7d-41f8-a3a4-2921ca8ed22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../nn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c44e9e-26c8-45b6-95de-83aa2f1947fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['x'].values\n",
    "y = data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58eec287-5ffd-407c-8920-69d45a5cf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7af9d746-6a12-4276-86dd-77eebb5f6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self):\n",
    "        self.w1 = random.random()\n",
    "        self.b = random.random()\n",
    "        self.w2 = random.random()\n",
    "        self.lr = 0.01\n",
    "        \n",
    "    def tanh(self,z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def forward(self,x):## x-> scalar, one data point\n",
    "        self.z1 = self.w1*x\n",
    "        self.h1 = self.tanh(self.z1)\n",
    "        self.z2 = (self.w2*self.h1)+self.b\n",
    "        self.h2 = self.tanh(self.z2)\n",
    "        return self.h2\n",
    "    \n",
    "    def backward(self,x,y):\n",
    "        error = (y-self.h2)\n",
    "        grad_act_l2 = (1-self.tanh(self.z2)**2)\n",
    "        grad_act_l1 = (1-self.tanh(self.z1)**2)\n",
    "        \n",
    "        grad_w1 = -1*(grad_act_l1)*x*self.w2*(grad_act_l2)*error\n",
    "        grad_w2 = -1*(grad_act_l2)*self.h1*error\n",
    "        grad_b = -1*(grad_act_l2)*error\n",
    "        \n",
    "        ## Apply Gradient Descent\n",
    "        \n",
    "        self.w1 = self.w1 - self.lr*grad_w1\n",
    "        self.w2 = self.w2 - self.lr*grad_w2\n",
    "        self.b = self.b - self.lr*grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e490b89-cf02-48a0-8b48-e5cdb6047dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c1f48c-96af-42a4-9795-f6f499660980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The w1 is 0.7779787982788374, w2 is 0.18373682364514954 and b is 0.5715411087422217, before the training\n"
     ]
    }
   ],
   "source": [
    "print(f\"The w1 is {network.w1}, w2 is {network.w2} and b is {network.b}, before the training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01587d6-f468-44c0-995b-fbee9c8703d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch: 1 is: 3886.34591195102\n",
      "Loss after epoch: 2 is: 3885.433514366245\n",
      "Loss after epoch: 3 is: 3885.184652229801\n",
      "Loss after epoch: 4 is: 3885.069240852518\n",
      "Loss after epoch: 5 is: 3885.0028096916367\n",
      "Loss after epoch: 6 is: 3884.9596847555867\n",
      "Loss after epoch: 7 is: 3884.929441890661\n",
      "Loss after epoch: 8 is: 3884.907056443429\n",
      "Loss after epoch: 9 is: 3884.889811112882\n",
      "Loss after epoch: 10 is: 3884.876109246789\n",
      "The w1 is 2.2497160597691, w2 is 3.45268228836755 and b is -0.11987448809306131, after the training\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for (x,actual) in zip(X,y):\n",
    "        pred = network.forward(x)\n",
    "        loss = 0.5*(actual-pred)**2\n",
    "        network.backward(x,actual)\n",
    "    print(f\"Loss after epoch: {epoch+1} is: {loss}\")\n",
    "print(f\"The w1 is {network.w1}, w2 is {network.w2} and b is {network.b}, after the training\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a05b5f-9197-4446-b34e-d62d3c86c10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8c768-61ab-4ed6-9163-4889a6cfbab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
