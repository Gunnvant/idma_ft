{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50eb9821-293e-438f-9563-98e52029ef21",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "**Backpropagation Algorithm**\n",
    "\n",
    "This notebook illustrates the backprop algorithm for a relatively simple dataset. The network architecture is as follows:\n",
    "\n",
    "1. $z_1 = w_1x$\n",
    "2. $a_1 = tanh(z_1)$\n",
    "3. $z_2 = w_2a_1+b$\n",
    "4. $y =tanh(z_2)$\n",
    "5. $C = \\frac{1}{2}(y-t)^2$\n",
    "\n",
    "Now the gradients needed would be for the following parameters:\n",
    "\n",
    "1. $w_1$\n",
    "2. $w_2$\n",
    "3. $b$\n",
    "\n",
    "\n",
    "The following gradient results will be needed:\n",
    "- $$\\frac{\\partial(C)}{\\partial(w_1)}=\\frac{\\partial z_1}{\\partial w_1}*\\frac{\\partial f_1}{\\partial z_1}*\\frac{\\partial z_2}{\\partial f_1}*\\frac{\\partial C}{\\partial z_2}= x*(1-tanh^2(z_1))*w_2*(tanh(z_2)-t)(1-tanh^2(z_2))$$\n",
    "\n",
    "- $$\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}*\\frac{\\partial C}{\\partial z_2} = tanh(z_1)*(tanh(z_2)-t)(1-tanh^2(z_2)$$\n",
    "\n",
    "- $$\\frac{\\partial C}{\\partial b} = \\frac{\\partial z2}{\\partial b}*\\frac{\\partial C}{\\partial z_2} = 1*(tanh(z_2)-t)(1-tanh^2(z_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ca4cc1-bad9-4e62-ad0b-539e8bc11d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/nn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85063d9-f502-4372-b98d-ee18937ba629",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['x'].values\n",
    "y = data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5a77dc-6883-4110-8bb2-860ecc685e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NN():\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.b = 1\n",
    "        self.eta = 0.01\n",
    "    def forward(self,x):\n",
    "        self.z1 = self.w1*x\n",
    "        self.a1 = math.tanh(self.z1)\n",
    "        self.z2 = self.w1*self.a1+self.b\n",
    "        y = math.tanh(self.z2)\n",
    "        return y \n",
    "    def backward(self,x,y):\n",
    "        grad_w1 = x*(1-math.tanh(self.z1)**2)*self.w2*(math.tanh(self.z2)-y)*(1-math.tanh(self.z2)**2)\n",
    "        grad_w2 = math.tanh(self.z1)*(math.tanh(self.z2)-y)*(1-math.tanh(self.z2)**2)\n",
    "        grad_b = (math.tanh(self.z2)-y)*(1-math.tanh(self.z2)**2)\n",
    "        self.w1 = self.w1-self.eta*grad_w1\n",
    "        self.w2 = self.w2-self.eta*grad_w2\n",
    "        self.b = self.b -self.eta*grad_b     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9acece-8095-4a31-8b03-a9a10b2ecbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN()\n",
    "model.w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd34e7a-4b93-4a7b-b2a9-a9ed91c5d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 0.78, actual value is 3.34\n",
      "Prediction is 0.95, actual value is 78.46\n",
      "Prediction is 0.15, actual value is -98.71\n",
      "Prediction is -0.73, actual value is -57.87\n",
      "Prediction is -0.8, actual value is -36.3\n",
      "Prediction is -0.95, actual value is -124.09\n",
      "Prediction is -0.95, actual value is -73.2\n",
      "Prediction is -0.91, actual value is -34.58\n",
      "Prediction is 0.5, actual value is 34.09\n",
      "Prediction is -0.97, actual value is -101.13\n",
      "Prediction is -0.97, actual value is -93.24\n",
      "Prediction is -0.88, actual value is -23.1\n",
      "Prediction is 0.75, actual value is 38.04\n",
      "Prediction is 0.86, actual value is 39.76\n",
      "Prediction is -0.97, actual value is -89.14\n",
      "Prediction is -0.02, actual value is 3.34\n",
      "Prediction is 0.94, actual value is 78.46\n",
      "Prediction is -0.97, actual value is -98.71\n",
      "Prediction is -0.97, actual value is -57.87\n",
      "Prediction is -0.95, actual value is -36.3\n",
      "Prediction is -0.98, actual value is -124.09\n",
      "Prediction is -0.98, actual value is -73.2\n",
      "Prediction is -0.96, actual value is -34.58\n",
      "Prediction is 0.85, actual value is 34.09\n",
      "Prediction is -0.98, actual value is -101.13\n",
      "Prediction is -0.98, actual value is -93.24\n",
      "Prediction is -0.93, actual value is -23.1\n",
      "Prediction is 0.9, actual value is 38.04\n",
      "Prediction is 0.92, actual value is 39.76\n",
      "Prediction is -0.98, actual value is -89.14\n",
      "Prediction is 0.01, actual value is 3.34\n",
      "Prediction is 0.96, actual value is 78.46\n",
      "Prediction is -0.98, actual value is -98.71\n",
      "Prediction is -0.98, actual value is -57.87\n",
      "Prediction is -0.97, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.97, actual value is -34.58\n",
      "Prediction is 0.91, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.95, actual value is -23.1\n",
      "Prediction is 0.93, actual value is 38.04\n",
      "Prediction is 0.95, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.04, actual value is 3.34\n",
      "Prediction is 0.97, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.98, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.98, actual value is -34.58\n",
      "Prediction is 0.94, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.96, actual value is -23.1\n",
      "Prediction is 0.95, actual value is 38.04\n",
      "Prediction is 0.96, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.07, actual value is 3.34\n",
      "Prediction is 0.98, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.98, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.98, actual value is -34.58\n",
      "Prediction is 0.95, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.97, actual value is -23.1\n",
      "Prediction is 0.96, actual value is 38.04\n",
      "Prediction is 0.97, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.09, actual value is 3.34\n",
      "Prediction is 0.98, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.99, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.99, actual value is -34.58\n",
      "Prediction is 0.96, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.97, actual value is -23.1\n",
      "Prediction is 0.97, actual value is 38.04\n",
      "Prediction is 0.97, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.12, actual value is 3.34\n",
      "Prediction is 0.98, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.99, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.99, actual value is -34.58\n",
      "Prediction is 0.97, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.97, actual value is -23.1\n",
      "Prediction is 0.97, actual value is 38.04\n",
      "Prediction is 0.98, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.14, actual value is 3.34\n",
      "Prediction is 0.99, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.99, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.99, actual value is -34.58\n",
      "Prediction is 0.97, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.98, actual value is -23.1\n",
      "Prediction is 0.98, actual value is 38.04\n",
      "Prediction is 0.98, actual value is 39.76\n",
      "Prediction is -0.99, actual value is -89.14\n",
      "Prediction is 0.16, actual value is 3.34\n",
      "Prediction is 0.99, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.99, actual value is -36.3\n",
      "Prediction is -0.99, actual value is -124.09\n",
      "Prediction is -0.99, actual value is -73.2\n",
      "Prediction is -0.99, actual value is -34.58\n",
      "Prediction is 0.98, actual value is 34.09\n",
      "Prediction is -0.99, actual value is -101.13\n",
      "Prediction is -0.99, actual value is -93.24\n",
      "Prediction is -0.98, actual value is -23.1\n",
      "Prediction is 0.98, actual value is 38.04\n",
      "Prediction is 0.98, actual value is 39.76\n",
      "Prediction is -1.0, actual value is -89.14\n",
      "Prediction is 0.17, actual value is 3.34\n",
      "Prediction is 0.99, actual value is 78.46\n",
      "Prediction is -0.99, actual value is -98.71\n",
      "Prediction is -0.99, actual value is -57.87\n",
      "Prediction is -0.99, actual value is -36.3\n",
      "Prediction is -1.0, actual value is -124.09\n",
      "Prediction is -1.0, actual value is -73.2\n",
      "Prediction is -0.99, actual value is -34.58\n",
      "Prediction is 0.98, actual value is 34.09\n",
      "Prediction is -1.0, actual value is -101.13\n",
      "Prediction is -1.0, actual value is -93.24\n",
      "Prediction is -0.98, actual value is -23.1\n",
      "Prediction is 0.98, actual value is 38.04\n",
      "Prediction is 0.98, actual value is 39.76\n",
      "Prediction is -1.0, actual value is -89.14\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for X,Y in zip(x,y):\n",
    "        pred = model.forward(X)\n",
    "        print(f\"Prediction is {round(pred,2)}, actual value is {Y.round(2)}\")\n",
    "        model.backward(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cbd10-637e-4b02-a97b-e98d361b002d",
   "metadata": {},
   "source": [
    "### Using autograd libraries to compute gradients\n",
    "\n",
    "- Implement Gradient Descent using auto-grad\n",
    "- Estimate Linear and Logistic Regression using auto-grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2634d0-361f-44ac-bd74-f2c0ff420e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80afe6a-7c53-4dd5-93cc-e13dbd18356c",
   "metadata": {},
   "source": [
    "Minmize $X^2+4X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38db9fe1-0670-4a62-b53f-c82e87dcda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ded0f74-a4f3-4c6c-98cf-2504da1769ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x ### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de08a36-eee3-4f57-98bf-08710f1c8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb3ad24-6dbf-4b93-8609-64dfe256ae0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Grad of x at 0 wrt z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f793a8-94b6-4bb2-973c-ee0cbfc20036",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a125d92-5811-406b-a24f-2115c2cf0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02b4add-f334-4e16-99f1-508330519124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb1640e-775d-449c-997d-e044b15df5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83072cae-edc8-4d7b-81da-4b2a84633d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea003ef4-dc4c-48f9-a047-7a0eb638b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z 0.0, x: -0.03999999910593033\n",
      "Z -0.15839999914169312, x: -0.07919999957084656\n",
      "Z -0.31052735447883606, x: -0.11761599779129028\n",
      "Z -0.4566304683685303, x: -0.15526367723941803\n",
      "Z -0.5969479084014893, x: -0.19215840101242065\n",
      "Z -0.7317087650299072, x: -0.22831523418426514\n",
      "Z -0.8611330986022949, x: -0.2637489140033722\n",
      "Z -0.9854321479797363, x: -0.29847392439842224\n",
      "Z -1.104809045791626, x: -0.3325044512748718\n",
      "Z -1.2194585800170898, x: -0.3658543527126312\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(10):\n",
    "    z=x*x+4*x\n",
    "    z.backward() ### dz/dx\n",
    "    with torch.no_grad(): ##Disables any gradient computation\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51357e05-96d2-48e7-a45a-bcb2321f3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autodiff\n",
    "### xy=750=>x=750/y\n",
    "## x+10y Minimize this\n",
    "### 750/y+y*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be388e75-807d-4313-9830-ce6f949af644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: 760.0, x: 8.399999618530273\n",
      "Z: 173.2857208251953, x: 8.406291961669922\n",
      "Z: 173.28179931640625, x: 8.41242504119873\n",
      "Z: 173.27809143066406, x: 8.418403625488281\n",
      "Z: 173.27456665039062, x: 8.42423152923584\n",
      "Z: 173.27120971679688, x: 8.429913520812988\n",
      "Z: 173.2680206298828, x: 8.435453414916992\n",
      "Z: 173.26498413085938, x: 8.4408540725708\n",
      "Z: 173.26211547851562, x: 8.446120262145996\n",
      "Z: 173.25936889648438, x: 8.451254844665527\n",
      "Z: 173.25677490234375, x: 8.45626163482666\n",
      "Z: 173.25428771972656, x: 8.46114444732666\n",
      "Z: 173.25193786621094, x: 8.465906143188477\n",
      "Z: 173.24969482421875, x: 8.470550537109375\n",
      "Z: 173.24755859375, x: 8.475079536437988\n",
      "Z: 173.24554443359375, x: 8.479496955871582\n",
      "Z: 173.2436065673828, x: 8.483805656433105\n",
      "Z: 173.2417755126953, x: 8.488008499145508\n",
      "Z: 173.24002075195312, x: 8.492108345031738\n",
      "Z: 173.23837280273438, x: 8.496108055114746\n",
      "Z: 173.23678588867188, x: 8.500009536743164\n",
      "Z: 173.23529052734375, x: 8.503815650939941\n",
      "Z: 173.23385620117188, x: 8.507528305053711\n",
      "Z: 173.2324981689453, x: 8.511151313781738\n",
      "Z: 173.231201171875, x: 8.51468563079834\n",
      "Z: 173.22998046875, x: 8.518134117126465\n",
      "Z: 173.22879028320312, x: 8.521498680114746\n",
      "Z: 173.22767639160156, x: 8.524782180786133\n",
      "Z: 173.22659301757812, x: 8.527985572814941\n",
      "Z: 173.2255859375, x: 8.531111717224121\n",
      "Z: 173.22463989257812, x: 8.534162521362305\n",
      "Z: 173.22372436523438, x: 8.537138938903809\n",
      "Z: 173.22283935546875, x: 8.540043830871582\n",
      "Z: 173.2220001220703, x: 8.542879104614258\n",
      "Z: 173.22120666503906, x: 8.545645713806152\n",
      "Z: 173.220458984375, x: 8.548345565795898\n",
      "Z: 173.2197265625, x: 8.550980567932129\n",
      "Z: 173.21905517578125, x: 8.553552627563477\n",
      "Z: 173.2183837890625, x: 8.556062698364258\n",
      "Z: 173.2177734375, x: 8.558512687683105\n",
      "Z: 173.21717834472656, x: 8.560904502868652\n",
      "Z: 173.21661376953125, x: 8.563239097595215\n",
      "Z: 173.216064453125, x: 8.56551742553711\n",
      "Z: 173.21556091308594, x: 8.567741394042969\n",
      "Z: 173.21507263183594, x: 8.569912910461426\n",
      "Z: 173.214599609375, x: 8.57203197479248\n",
      "Z: 173.21417236328125, x: 8.574100494384766\n",
      "Z: 173.2137451171875, x: 8.576120376586914\n",
      "Z: 173.21331787109375, x: 8.578091621398926\n",
      "Z: 173.21295166015625, x: 8.580016136169434\n",
      "Z: 173.21258544921875, x: 8.581894874572754\n",
      "Z: 173.2122344970703, x: 8.58372974395752\n",
      "Z: 173.2119140625, x: 8.58552074432373\n",
      "Z: 173.21157836914062, x: 8.587268829345703\n",
      "Z: 173.21127319335938, x: 8.58897590637207\n",
      "Z: 173.21099853515625, x: 8.590642929077148\n",
      "Z: 173.21072387695312, x: 8.592269897460938\n",
      "Z: 173.21044921875, x: 8.59385871887207\n",
      "Z: 173.210205078125, x: 8.595409393310547\n",
      "Z: 173.20997619628906, x: 8.596923828125\n",
      "Z: 173.20974731445312, x: 8.598402976989746\n",
      "Z: 173.20953369140625, x: 8.599846839904785\n",
      "Z: 173.20932006835938, x: 8.601256370544434\n",
      "Z: 173.20913696289062, x: 8.602632522583008\n",
      "Z: 173.2089385986328, x: 8.603976249694824\n",
      "Z: 173.20877075195312, x: 8.6052885055542\n",
      "Z: 173.20858764648438, x: 8.60657024383545\n",
      "Z: 173.20843505859375, x: 8.607821464538574\n",
      "Z: 173.20828247070312, x: 8.60904312133789\n",
      "Z: 173.2081298828125, x: 8.610236167907715\n",
      "Z: 173.20799255371094, x: 8.611401557922363\n",
      "Z: 173.20785522460938, x: 8.612539291381836\n",
      "Z: 173.20773315429688, x: 8.61365032196045\n",
      "Z: 173.20761108398438, x: 8.61473560333252\n",
      "Z: 173.20748901367188, x: 8.615795135498047\n",
      "Z: 173.20736694335938, x: 8.616829872131348\n",
      "Z: 173.207275390625, x: 8.617840766906738\n",
      "Z: 173.2071533203125, x: 8.618827819824219\n",
      "Z: 173.2070770263672, x: 8.619791030883789\n",
      "Z: 173.20697021484375, x: 8.620732307434082\n",
      "Z: 173.20687866210938, x: 8.621651649475098\n",
      "Z: 173.20680236816406, x: 8.622549057006836\n",
      "Z: 173.20672607421875, x: 8.623425483703613\n",
      "Z: 173.20664978027344, x: 8.624281883239746\n",
      "Z: 173.20657348632812, x: 8.625118255615234\n",
      "Z: 173.20651245117188, x: 8.625934600830078\n",
      "Z: 173.20645141601562, x: 8.626731872558594\n",
      "Z: 173.20639038085938, x: 8.627511024475098\n",
      "Z: 173.20632934570312, x: 8.628271102905273\n",
      "Z: 173.20626831054688, x: 8.629014015197754\n",
      "Z: 173.20620727539062, x: 8.629739761352539\n",
      "Z: 173.20614624023438, x: 8.630448341369629\n",
      "Z: 173.20611572265625, x: 8.63114070892334\n",
      "Z: 173.2060546875, x: 8.631816864013672\n",
      "Z: 173.20602416992188, x: 8.632476806640625\n",
      "Z: 173.2059783935547, x: 8.633121490478516\n",
      "Z: 173.2059326171875, x: 8.633750915527344\n",
      "Z: 173.20590209960938, x: 8.634366035461426\n",
      "Z: 173.2058563232422, x: 8.634966850280762\n",
      "Z: 173.20582580566406, x: 8.635553359985352\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=(750/x)+x*10\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z: {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0d2807-c717-464c-9834-b93088ae5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reg=pd.read_csv(\"./data/regression.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e804632d-e967-402a-869e-33ae686ec84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1da66b63-27f0-4cce-8fca-288df8cab9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### mpg=b0+b1*cyl ## as a matrix product?\n",
    "### dloss/db0, dloss/db1\n",
    "#### loss=f(b0,b1)\n",
    "##loss=eq\n",
    "##loss.backward()\n",
    "##b0.grad\n",
    "##b1.grad\n",
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36051e23-3124-4a1c-8c50-81d783b4673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## W dim?=> dloss/dW,dloss/db\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3d9b0b-bb22-4050-832f-796d321078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85019d7d-bc7c-4bc2-8574-e7cd23693d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1232.7423724703717, W: [[1.897617]], b: [0.70098984]\n",
      "Loss: 299.83267440683056, W: [[3.0871732]], b: [0.42572457]\n",
      "Loss: 200.88668794171477, W: [[3.4652]], b: [0.28614816]\n",
      "Loss: 189.93863059455396, W: [[3.5796304]], b: [0.19073406]\n",
      "Loss: 188.2774799440126, W: [[3.608451]], b: [0.10975139]\n",
      "Loss: 187.59797955950813, W: [[3.609477]], b: [0.03354244]\n",
      "Loss: 187.02379210418584, W: [[3.6014888]], b: [-0.04103003]\n",
      "Loss: 186.46244312247816, W: [[3.5905867]], b: [-0.11498526]\n",
      "Loss: 185.9041695255456, W: [[3.5787525]], b: [-0.18865451]\n",
      "Loss: 185.34793517195828, W: [[3.56663]], b: [-0.2621455]\n",
      "Loss: 184.79362537959506, W: [[3.5544276]], b: [-0.33549333]\n",
      "Loss: 184.24121948872394, W: [[3.542214]], b: [-0.40870962]\n",
      "Loss: 183.6907144867814, W: [[3.5300107]], b: [-0.48179823]\n",
      "Loss: 183.14209987618685, W: [[3.5178246]], b: [-0.5547606]\n",
      "Loss: 182.59536999680304, W: [[3.5056586]], b: [-0.62759733]\n",
      "Loss: 182.05051716281997, W: [[3.493513]], b: [-0.7003088]\n",
      "Loss: 181.50753823176538, W: [[3.4813886]], b: [-0.7728952]\n",
      "Loss: 180.9664252538518, W: [[3.4692845]], b: [-0.84535676]\n",
      "Loss: 180.4271708879964, W: [[3.4572015]], b: [-0.91769373]\n",
      "Loss: 179.88976941942985, W: [[3.445139]], b: [-0.9899063]\n",
      "Loss: 179.35421354743391, W: [[3.4330976]], b: [-1.0619948]\n",
      "Loss: 178.82049834660572, W: [[3.4210768]], b: [-1.1339593]\n",
      "Loss: 178.288617152073, W: [[3.4090767]], b: [-1.2058]\n",
      "Loss: 177.75856362218636, W: [[3.3970969]], b: [-1.2775172]\n",
      "Loss: 177.2303316122628, W: [[3.385138]], b: [-1.3491111]\n",
      "Loss: 176.70391492427797, W: [[3.3731992]], b: [-1.4205818]\n",
      "Loss: 176.1793053042401, W: [[3.3612814]], b: [-1.4919298]\n",
      "Loss: 175.65649891723322, W: [[3.349384]], b: [-1.563155]\n",
      "Loss: 175.13548864952196, W: [[3.337507]], b: [-1.6342578]\n",
      "Loss: 174.61626893512442, W: [[3.3256505]], b: [-1.7052383]\n",
      "Loss: 174.09883336668403, W: [[3.3138144]], b: [-1.7760968]\n",
      "Loss: 173.58317531333356, W: [[3.3019989]], b: [-1.8468335]\n",
      "Loss: 173.06928876695056, W: [[3.2902033]], b: [-1.9174484]\n",
      "Loss: 172.5571692112242, W: [[3.2784278]], b: [-1.987942]\n",
      "Loss: 172.04680745640033, W: [[3.266673]], b: [-2.0583143]\n",
      "Loss: 171.53820075842353, W: [[3.2549386]], b: [-2.1285658]\n",
      "Loss: 171.0313413876749, W: [[3.243224]], b: [-2.1986964]\n",
      "Loss: 170.526221905511, W: [[3.2315295]], b: [-2.2687063]\n",
      "Loss: 170.02284001863066, W: [[3.219855]], b: [-2.3385959]\n",
      "Loss: 169.52118672017082, W: [[3.208201]], b: [-2.4083652]\n",
      "Loss: 169.02125875349213, W: [[3.1965666]], b: [-2.4780147]\n",
      "Loss: 168.5230462800613, W: [[3.1849525]], b: [-2.5475445]\n",
      "Loss: 168.02654575720064, W: [[3.1733582]], b: [-2.6169546]\n",
      "Loss: 167.53175288216016, W: [[3.161784]], b: [-2.6862454]\n",
      "Loss: 167.03865790854363, W: [[3.1502297]], b: [-2.755417]\n",
      "Loss: 166.5472591079045, W: [[3.1386952]], b: [-2.8244698]\n",
      "Loss: 166.0575485318241, W: [[3.1271806]], b: [-2.8934038]\n",
      "Loss: 165.56952018390007, W: [[3.1156855]], b: [-2.9622192]\n",
      "Loss: 165.08316748101146, W: [[3.1042104]], b: [-3.0309165]\n",
      "Loss: 164.5984870037397, W: [[3.092755]], b: [-3.0994954]\n",
      "Loss: 164.11547118251454, W: [[3.081319]], b: [-3.1679566]\n",
      "Loss: 163.6341152344899, W: [[3.0699034]], b: [-3.2363]\n",
      "Loss: 163.15441331632536, W: [[3.058507]], b: [-3.3045259]\n",
      "Loss: 162.67635977646893, W: [[3.04713]], b: [-3.3726344]\n",
      "Loss: 162.19994801735118, W: [[3.035773]], b: [-3.440626]\n",
      "Loss: 161.72517411751306, W: [[3.024435]], b: [-3.5085006]\n",
      "Loss: 161.25203043317086, W: [[3.0131168]], b: [-3.5762584]\n",
      "Loss: 160.7805132378912, W: [[3.0018182]], b: [-3.6439]\n",
      "Loss: 160.31061584244432, W: [[2.9905386]], b: [-3.711425]\n",
      "Loss: 159.84233153925376, W: [[2.9792786]], b: [-3.778834]\n",
      "Loss: 159.37565709349144, W: [[2.9680383]], b: [-3.8461273]\n",
      "Loss: 158.91058602131739, W: [[2.956817]], b: [-3.9133046]\n",
      "Loss: 158.44711445021844, W: [[2.945615]], b: [-3.9803665]\n",
      "Loss: 157.985234197791, W: [[2.9344325]], b: [-4.047313]\n",
      "Loss: 157.52493896576024, W: [[2.923269]], b: [-4.114145]\n",
      "Loss: 157.06622673401705, W: [[2.9121249]], b: [-4.1808615]\n",
      "Loss: 156.6090911005042, W: [[2.9009995]], b: [-4.247463]\n",
      "Loss: 156.15352680268458, W: [[2.8898935]], b: [-4.3139505]\n",
      "Loss: 155.69952648036684, W: [[2.8788066]], b: [-4.3803234]\n",
      "Loss: 155.2470868349352, W: [[2.8677387]], b: [-4.4465823]\n",
      "Loss: 154.79620202098064, W: [[2.85669]], b: [-4.5127273]\n",
      "Loss: 154.3468666993843, W: [[2.8456604]], b: [-4.5787582]\n",
      "Loss: 153.8990761331185, W: [[2.8346496]], b: [-4.6446757]\n",
      "Loss: 153.4528227400199, W: [[2.8236578]], b: [-4.71048]\n",
      "Loss: 153.008101951876, W: [[2.8126848]], b: [-4.776171]\n",
      "Loss: 152.5649110193727, W: [[2.8017306]], b: [-4.841749]\n",
      "Loss: 152.12324286842718, W: [[2.7907953]], b: [-4.9072146]\n",
      "Loss: 151.6830917916303, W: [[2.7798786]], b: [-4.9725676]\n",
      "Loss: 151.2444519501432, W: [[2.768981]], b: [-5.037808]\n",
      "Loss: 150.8073206333793, W: [[2.758102]], b: [-5.1029363]\n",
      "Loss: 150.37168965921813, W: [[2.7472417]], b: [-5.1679525]\n",
      "Loss: 149.93755709143937, W: [[2.7364001]], b: [-5.232857]\n",
      "Loss: 149.50491404442397, W: [[2.7255774]], b: [-5.2976503]\n",
      "Loss: 149.0737586112068, W: [[2.714773]], b: [-5.362332]\n",
      "Loss: 148.64408443255357, W: [[2.703987]], b: [-5.4269023]\n",
      "Loss: 148.2158859500904, W: [[2.69322]], b: [-5.4913616]\n",
      "Loss: 147.7891597947958, W: [[2.682471]], b: [-5.55571]\n",
      "Loss: 147.36390114323044, W: [[2.6717408]], b: [-5.6199474]\n",
      "Loss: 146.9401043311992, W: [[2.6610289]], b: [-5.684075]\n",
      "Loss: 146.51776060478198, W: [[2.6503358]], b: [-5.748092]\n",
      "Loss: 146.0968681312072, W: [[2.6396606]], b: [-5.8119993]\n",
      "Loss: 145.67742129831717, W: [[2.629004]], b: [-5.8757963]\n",
      "Loss: 145.25941838907417, W: [[2.6183658]], b: [-5.9394836]\n",
      "Loss: 144.842851079416, W: [[2.607746]], b: [-6.003062]\n",
      "Loss: 144.42771300421208, W: [[2.5971441]], b: [-6.06653]\n",
      "Loss: 144.01400401552416, W: [[2.5865607]], b: [-6.1298895]\n",
      "Loss: 143.60171665149403, W: [[2.5759954]], b: [-6.19314]\n",
      "Loss: 143.19084449871173, W: [[2.565448]], b: [-6.256282]\n",
      "Loss: 142.7813838148943, W: [[2.5549192]], b: [-6.319315]\n",
      "Loss: 142.37333076478774, W: [[2.5444083]], b: [-6.38224]\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(100):\n",
    "    diff=y-torch.matmul(X.float(),W)+b\n",
    "    loss=sum(diff*diff)/y.shape[0]\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W.detach().numpy()}, b: {b.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9367d643-03ff-4e2a-859f-63211d040f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### linear classifier.\n",
    "### Can you estimate a linear classifier using autodiff\n",
    "### Loss for linear classifier?\n",
    "### log loss as a function of W and b, p=f(X,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c648fb49-1a49-4f98-812f-91c57b25f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_pregnant</th>\n",
       "      <th>Plasma_glucose</th>\n",
       "      <th>Blood_pres</th>\n",
       "      <th>Skin_thick</th>\n",
       "      <th>Serum_insu</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Diabetes_func</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No_pregnant  Plasma_glucose  Blood_pres  Skin_thick  Serum_insu   BMI  \\\n",
       "0            6             148          72          35           0  33.6   \n",
       "1            1              85          66          29           0  26.6   \n",
       "2            8             183          64           0           0  23.3   \n",
       "3            1              89          66          23          94  28.1   \n",
       "4            0             137          40          35         168  43.1   \n",
       "\n",
       "   Diabetes_func  Age  Class  \n",
       "0          0.627   50      1  \n",
       "1          0.351   31      0  \n",
       "2          0.672   32      1  \n",
       "3          0.167   21      0  \n",
       "4          2.288   33      1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls=pd.read_csv(\"./data/classification.csv\")\n",
    "cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1496d643-bfa1-4ee4-a443-4930d5bca9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cls[['No_pregnant']].values\n",
    "y=cls[['Class']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54f17e9-a211-4e1f-a63e-2fbad4bedf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss=−[𝑦𝑙𝑜𝑔(𝑝+tol)+(1−𝑦)𝑙𝑜𝑔(1−𝑝+tol)]\n",
    "### p=1/(1+e^-z)\n",
    "### z= XW+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d76d581-af74-4ca0-b5f9-7cf84d9b6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6268749237060547, W: tensor([[-1.6294]], requires_grad=True), b: tensor([1.6334], requires_grad=True)\n",
      "Loss: 2.603846788406372, W: tensor([[-1.6143]], requires_grad=True), b: tensor([1.6345], requires_grad=True)\n",
      "Loss: 2.5808122158050537, W: tensor([[-1.5991]], requires_grad=True), b: tensor([1.6356], requires_grad=True)\n",
      "Loss: 2.5577785968780518, W: tensor([[-1.5840]], requires_grad=True), b: tensor([1.6367], requires_grad=True)\n",
      "Loss: 2.5347530841827393, W: tensor([[-1.5689]], requires_grad=True), b: tensor([1.6377], requires_grad=True)\n",
      "Loss: 2.5117435455322266, W: tensor([[-1.5537]], requires_grad=True), b: tensor([1.6388], requires_grad=True)\n",
      "Loss: 2.4887564182281494, W: tensor([[-1.5386]], requires_grad=True), b: tensor([1.6398], requires_grad=True)\n",
      "Loss: 2.4657962322235107, W: tensor([[-1.5235]], requires_grad=True), b: tensor([1.6409], requires_grad=True)\n",
      "Loss: 2.4428679943084717, W: tensor([[-1.5084]], requires_grad=True), b: tensor([1.6419], requires_grad=True)\n",
      "Loss: 2.4199745655059814, W: tensor([[-1.4933]], requires_grad=True), b: tensor([1.6429], requires_grad=True)\n",
      "Loss: 2.3971192836761475, W: tensor([[-1.4782]], requires_grad=True), b: tensor([1.6438], requires_grad=True)\n",
      "Loss: 2.3743045330047607, W: tensor([[-1.4632]], requires_grad=True), b: tensor([1.6448], requires_grad=True)\n",
      "Loss: 2.3515331745147705, W: tensor([[-1.4481]], requires_grad=True), b: tensor([1.6457], requires_grad=True)\n",
      "Loss: 2.3288090229034424, W: tensor([[-1.4331]], requires_grad=True), b: tensor([1.6467], requires_grad=True)\n",
      "Loss: 2.306135892868042, W: tensor([[-1.4181]], requires_grad=True), b: tensor([1.6476], requires_grad=True)\n",
      "Loss: 2.2835190296173096, W: tensor([[-1.4031]], requires_grad=True), b: tensor([1.6485], requires_grad=True)\n",
      "Loss: 2.2609643936157227, W: tensor([[-1.3881]], requires_grad=True), b: tensor([1.6493], requires_grad=True)\n",
      "Loss: 2.2384793758392334, W: tensor([[-1.3732]], requires_grad=True), b: tensor([1.6502], requires_grad=True)\n",
      "Loss: 2.216071367263794, W: tensor([[-1.3582]], requires_grad=True), b: tensor([1.6510], requires_grad=True)\n",
      "Loss: 2.1937477588653564, W: tensor([[-1.3433]], requires_grad=True), b: tensor([1.6519], requires_grad=True)\n",
      "Loss: 2.1715164184570312, W: tensor([[-1.3285]], requires_grad=True), b: tensor([1.6527], requires_grad=True)\n",
      "Loss: 2.1493844985961914, W: tensor([[-1.3136]], requires_grad=True), b: tensor([1.6534], requires_grad=True)\n",
      "Loss: 2.1273586750030518, W: tensor([[-1.2989]], requires_grad=True), b: tensor([1.6542], requires_grad=True)\n",
      "Loss: 2.105445146560669, W: tensor([[-1.2841]], requires_grad=True), b: tensor([1.6549], requires_grad=True)\n",
      "Loss: 2.0836498737335205, W: tensor([[-1.2694]], requires_grad=True), b: tensor([1.6557], requires_grad=True)\n",
      "Loss: 2.0619781017303467, W: tensor([[-1.2547]], requires_grad=True), b: tensor([1.6564], requires_grad=True)\n",
      "Loss: 2.0404345989227295, W: tensor([[-1.2401]], requires_grad=True), b: tensor([1.6570], requires_grad=True)\n",
      "Loss: 2.019024133682251, W: tensor([[-1.2255]], requires_grad=True), b: tensor([1.6577], requires_grad=True)\n",
      "Loss: 1.9977508783340454, W: tensor([[-1.2110]], requires_grad=True), b: tensor([1.6583], requires_grad=True)\n",
      "Loss: 1.9766188859939575, W: tensor([[-1.1965]], requires_grad=True), b: tensor([1.6589], requires_grad=True)\n",
      "Loss: 1.955632209777832, W: tensor([[-1.1820]], requires_grad=True), b: tensor([1.6595], requires_grad=True)\n",
      "Loss: 1.9347947835922241, W: tensor([[-1.1677]], requires_grad=True), b: tensor([1.6601], requires_grad=True)\n",
      "Loss: 1.9141101837158203, W: tensor([[-1.1533]], requires_grad=True), b: tensor([1.6606], requires_grad=True)\n",
      "Loss: 1.8935818672180176, W: tensor([[-1.1390]], requires_grad=True), b: tensor([1.6611], requires_grad=True)\n",
      "Loss: 1.8732134103775024, W: tensor([[-1.1248]], requires_grad=True), b: tensor([1.6616], requires_grad=True)\n",
      "Loss: 1.8530091047286987, W: tensor([[-1.1107]], requires_grad=True), b: tensor([1.6621], requires_grad=True)\n",
      "Loss: 1.8329719305038452, W: tensor([[-1.0966]], requires_grad=True), b: tensor([1.6626], requires_grad=True)\n",
      "Loss: 1.8131057024002075, W: tensor([[-1.0825]], requires_grad=True), b: tensor([1.6630], requires_grad=True)\n",
      "Loss: 1.7934144735336304, W: tensor([[-1.0685]], requires_grad=True), b: tensor([1.6634], requires_grad=True)\n",
      "Loss: 1.7739014625549316, W: tensor([[-1.0546]], requires_grad=True), b: tensor([1.6637], requires_grad=True)\n",
      "Loss: 1.7545709609985352, W: tensor([[-1.0408]], requires_grad=True), b: tensor([1.6641], requires_grad=True)\n",
      "Loss: 1.7354265451431274, W: tensor([[-1.0270]], requires_grad=True), b: tensor([1.6644], requires_grad=True)\n",
      "Loss: 1.7164721488952637, W: tensor([[-1.0133]], requires_grad=True), b: tensor([1.6647], requires_grad=True)\n",
      "Loss: 1.6977115869522095, W: tensor([[-0.9996]], requires_grad=True), b: tensor([1.6649], requires_grad=True)\n",
      "Loss: 1.6791491508483887, W: tensor([[-0.9861]], requires_grad=True), b: tensor([1.6652], requires_grad=True)\n",
      "Loss: 1.6607885360717773, W: tensor([[-0.9726]], requires_grad=True), b: tensor([1.6654], requires_grad=True)\n",
      "Loss: 1.6426342725753784, W: tensor([[-0.9592]], requires_grad=True), b: tensor([1.6656], requires_grad=True)\n",
      "Loss: 1.624690055847168, W: tensor([[-0.9458]], requires_grad=True), b: tensor([1.6657], requires_grad=True)\n",
      "Loss: 1.6069602966308594, W: tensor([[-0.9326]], requires_grad=True), b: tensor([1.6658], requires_grad=True)\n",
      "Loss: 1.5894489288330078, W: tensor([[-0.9194]], requires_grad=True), b: tensor([1.6659], requires_grad=True)\n",
      "Loss: 1.5721606016159058, W: tensor([[-0.9063]], requires_grad=True), b: tensor([1.6660], requires_grad=True)\n",
      "Loss: 1.5550994873046875, W: tensor([[-0.8933]], requires_grad=True), b: tensor([1.6660], requires_grad=True)\n",
      "Loss: 1.5382696390151978, W: tensor([[-0.8804]], requires_grad=True), b: tensor([1.6660], requires_grad=True)\n",
      "Loss: 1.521675705909729, W: tensor([[-0.8676]], requires_grad=True), b: tensor([1.6660], requires_grad=True)\n",
      "Loss: 1.5053218603134155, W: tensor([[-0.8549]], requires_grad=True), b: tensor([1.6659], requires_grad=True)\n",
      "Loss: 1.4892120361328125, W: tensor([[-0.8423]], requires_grad=True), b: tensor([1.6658], requires_grad=True)\n",
      "Loss: 1.473351001739502, W: tensor([[-0.8298]], requires_grad=True), b: tensor([1.6657], requires_grad=True)\n",
      "Loss: 1.4577430486679077, W: tensor([[-0.8173]], requires_grad=True), b: tensor([1.6655], requires_grad=True)\n",
      "Loss: 1.4423918724060059, W: tensor([[-0.8050]], requires_grad=True), b: tensor([1.6653], requires_grad=True)\n",
      "Loss: 1.4273022413253784, W: tensor([[-0.7928]], requires_grad=True), b: tensor([1.6651], requires_grad=True)\n",
      "Loss: 1.4124778509140015, W: tensor([[-0.7807]], requires_grad=True), b: tensor([1.6649], requires_grad=True)\n",
      "Loss: 1.3979231119155884, W: tensor([[-0.7688]], requires_grad=True), b: tensor([1.6646], requires_grad=True)\n",
      "Loss: 1.3836416006088257, W: tensor([[-0.7569]], requires_grad=True), b: tensor([1.6642], requires_grad=True)\n",
      "Loss: 1.369637370109558, W: tensor([[-0.7452]], requires_grad=True), b: tensor([1.6639], requires_grad=True)\n",
      "Loss: 1.3559139966964722, W: tensor([[-0.7336]], requires_grad=True), b: tensor([1.6635], requires_grad=True)\n",
      "Loss: 1.3424750566482544, W: tensor([[-0.7221]], requires_grad=True), b: tensor([1.6630], requires_grad=True)\n",
      "Loss: 1.3293241262435913, W: tensor([[-0.7107]], requires_grad=True), b: tensor([1.6626], requires_grad=True)\n",
      "Loss: 1.3164639472961426, W: tensor([[-0.6995]], requires_grad=True), b: tensor([1.6621], requires_grad=True)\n",
      "Loss: 1.3038979768753052, W: tensor([[-0.6884]], requires_grad=True), b: tensor([1.6615], requires_grad=True)\n",
      "Loss: 1.2916287183761597, W: tensor([[-0.6774]], requires_grad=True), b: tensor([1.6609], requires_grad=True)\n",
      "Loss: 1.2796586751937866, W: tensor([[-0.6666]], requires_grad=True), b: tensor([1.6603], requires_grad=True)\n",
      "Loss: 1.267990231513977, W: tensor([[-0.6559]], requires_grad=True), b: tensor([1.6597], requires_grad=True)\n",
      "Loss: 1.2566252946853638, W: tensor([[-0.6454]], requires_grad=True), b: tensor([1.6590], requires_grad=True)\n",
      "Loss: 1.2455655336380005, W: tensor([[-0.6350]], requires_grad=True), b: tensor([1.6583], requires_grad=True)\n",
      "Loss: 1.2348119020462036, W: tensor([[-0.6248]], requires_grad=True), b: tensor([1.6575], requires_grad=True)\n",
      "Loss: 1.2243655920028687, W: tensor([[-0.6147]], requires_grad=True), b: tensor([1.6567], requires_grad=True)\n",
      "Loss: 1.2142270803451538, W: tensor([[-0.6048]], requires_grad=True), b: tensor([1.6559], requires_grad=True)\n",
      "Loss: 1.2043966054916382, W: tensor([[-0.5950]], requires_grad=True), b: tensor([1.6550], requires_grad=True)\n",
      "Loss: 1.1948736906051636, W: tensor([[-0.5854]], requires_grad=True), b: tensor([1.6541], requires_grad=True)\n",
      "Loss: 1.1856578588485718, W: tensor([[-0.5760]], requires_grad=True), b: tensor([1.6531], requires_grad=True)\n",
      "Loss: 1.1767477989196777, W: tensor([[-0.5667]], requires_grad=True), b: tensor([1.6521], requires_grad=True)\n",
      "Loss: 1.1681420803070068, W: tensor([[-0.5576]], requires_grad=True), b: tensor([1.6511], requires_grad=True)\n",
      "Loss: 1.1598385572433472, W: tensor([[-0.5487]], requires_grad=True), b: tensor([1.6500], requires_grad=True)\n",
      "Loss: 1.151834487915039, W: tensor([[-0.5400]], requires_grad=True), b: tensor([1.6489], requires_grad=True)\n",
      "Loss: 1.144127368927002, W: tensor([[-0.5314]], requires_grad=True), b: tensor([1.6477], requires_grad=True)\n",
      "Loss: 1.1367132663726807, W: tensor([[-0.5230]], requires_grad=True), b: tensor([1.6465], requires_grad=True)\n",
      "Loss: 1.1295884847640991, W: tensor([[-0.5148]], requires_grad=True), b: tensor([1.6453], requires_grad=True)\n",
      "Loss: 1.122748613357544, W: tensor([[-0.5067]], requires_grad=True), b: tensor([1.6441], requires_grad=True)\n",
      "Loss: 1.116188645362854, W: tensor([[-0.4989]], requires_grad=True), b: tensor([1.6428], requires_grad=True)\n",
      "Loss: 1.1099038124084473, W: tensor([[-0.4912]], requires_grad=True), b: tensor([1.6414], requires_grad=True)\n",
      "Loss: 1.103887915611267, W: tensor([[-0.4837]], requires_grad=True), b: tensor([1.6400], requires_grad=True)\n",
      "Loss: 1.098134994506836, W: tensor([[-0.4764]], requires_grad=True), b: tensor([1.6386], requires_grad=True)\n",
      "Loss: 1.0926387310028076, W: tensor([[-0.4692]], requires_grad=True), b: tensor([1.6372], requires_grad=True)\n",
      "Loss: 1.0873923301696777, W: tensor([[-0.4623]], requires_grad=True), b: tensor([1.6357], requires_grad=True)\n",
      "Loss: 1.0823887586593628, W: tensor([[-0.4555]], requires_grad=True), b: tensor([1.6342], requires_grad=True)\n",
      "Loss: 1.0776206254959106, W: tensor([[-0.4489]], requires_grad=True), b: tensor([1.6326], requires_grad=True)\n",
      "Loss: 1.0730806589126587, W: tensor([[-0.4425]], requires_grad=True), b: tensor([1.6310], requires_grad=True)\n",
      "Loss: 1.0687607526779175, W: tensor([[-0.4362]], requires_grad=True), b: tensor([1.6294], requires_grad=True)\n",
      "Loss: 1.064652919769287, W: tensor([[-0.4301]], requires_grad=True), b: tensor([1.6278], requires_grad=True)\n",
      "Loss: 1.0607492923736572, W: tensor([[-0.4243]], requires_grad=True), b: tensor([1.6261], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)\n",
    "tol=0.0000000001\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=torch.matmul(X.float(),W)+b\n",
    "    p=1.0/(1+torch.exp(-z))\n",
    "    loss=-(y*torch.log(p+tol)+(1-y)*torch.log(1-p+tol)).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d784aed-c43c-4a60-8ebe-e278937225be",
   "metadata": {},
   "source": [
    "## Using tensorflow to compute gradients \n",
    "\n",
    "$y = x^2 +4x$\n",
    "\n",
    "$\\frac{dy}{dx} = 2x+4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09d53aa3-c017-4833-97b3-4c363fd39ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 16:50:52.588684: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-23 16:50:52.588799: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f13e850-0e34-42e3-9dd9-f12c9eeadcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = x**2+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dce69b9-2b75-4275-847d-483ae3f3689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y, x) ## compute dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7353647-f5a5-48c4-af01-150b62d76f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "639a3595-5117-4fcb-bf0a-6b3b208da5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e7824-7077-415f-b4e6-aa99b3fe0ede",
   "metadata": {},
   "source": [
    "Write the gradient descent using ```GradientTape()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ae3ed39-acf1-4aa4-a8d9-d7fe7db03578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4\n",
      "-0.72\n",
      "-0.9760001\n",
      "-1.1808001\n",
      "-1.34464\n",
      "-1.4757121\n",
      "-1.5805696\n",
      "-1.6644558\n",
      "-1.7315646\n",
      "-1.7852517\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.0)\n",
    "lr = 0.1\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x**2+4*x\n",
    "    grad = tape.gradient(y,x)\n",
    "    x.assign_sub(lr*grad)\n",
    "    print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943b60d-7903-48fe-bd12-edbfda48109b",
   "metadata": {},
   "source": [
    "### Linear Regression with basic tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "551a7a86-463a-4fe6-ada4-f15e9a9c2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "2  18.0        8.0         318.0       150.0  3436.0          11.0  70.0   \n",
       "3  16.0        8.0         304.0       150.0  3433.0          12.0  70.0   \n",
       "4  17.0        8.0         302.0       140.0  3449.0          10.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "778cf3c2-0785-4b6c-aa84-4c76f032be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37815f60-cb6f-4d8b-9851-218d2e9259ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e11b8769-4f33-4300-94bf-9284b30cc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y):\n",
    "    return tf.reduce_mean(tf.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e24ce4c6-0502-447f-92e4-87df4d2fe2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(830.1917, shape=(), dtype=float32)\n",
      "tf.Tensor(184.82008, shape=(), dtype=float32)\n",
      "tf.Tensor(116.57598, shape=(), dtype=float32)\n",
      "tf.Tensor(109.228386, shape=(), dtype=float32)\n",
      "tf.Tensor(108.3068, shape=(), dtype=float32)\n",
      "tf.Tensor(108.06379, shape=(), dtype=float32)\n",
      "tf.Tensor(107.892845, shape=(), dtype=float32)\n",
      "tf.Tensor(107.730034, shape=(), dtype=float32)\n",
      "tf.Tensor(107.56855, shape=(), dtype=float32)\n",
      "tf.Tensor(107.40773, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal(shape=(1,1),dtype='float32'))\n",
    "B = tf.Variable(tf.random.normal(shape=(1,),dtype='float32'))\n",
    "lr = 0.01\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.reshape(X@W+B,shape=X.shape[0])\n",
    "        error = loss(y_pred,y)\n",
    "    dw,db = tape.gradient(error,[W,B])\n",
    "    W.assign_sub(lr*dw)\n",
    "    B.assign_sub(lr*db)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5cbf9a4-98e9-4bb8-8a0e-15bcb3e1aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class Excercise: Use tensorflow api to write the gradients for logistic regresssion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
