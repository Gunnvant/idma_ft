{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa7e0a4-42bc-4557-8600-3beab261ff1c",
   "metadata": {},
   "source": [
    "- Coding a neural network using only matrix multiplication\n",
    "- Writting a dataloader to be used in training\n",
    "- Creating a NN with high level classes and sequential api\n",
    "- Using an optimizer and pre-defined loss\n",
    "- Using tf and keras\n",
    "\n",
    "Data: \n",
    "- [mnist_train](https://drive.google.com/file/d/1bCVtZBPQcEz3AqkvrI1M69D-mcY6f5TK/view?usp=sharing)\n",
    "- [mnist_test](https://drive.google.com/file/d/1DrN5-afU-U6x5hMrUgpUaOA7wYZzXkmz/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e484f5-6f44-447d-885c-d91687b24087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2317f2-b117-474f-a528-7f3053b0a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path_mnist_train = \"/Users/gunnvantsaini/OneDrive/project_codes/content/dl_basics/vision/sony/data/mnist_train.csv\"\n",
    "local_path_mnist_test = \"/Users/gunnvantsaini/OneDrive/project_codes/content/dl_basics/vision/sony/data/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73b2cf-bc50-4316-97e3-198af2708e94",
   "metadata": {},
   "source": [
    "### Coding a neural network using only matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00641af5-fb6c-4ac8-b065-1f770dab02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv(local_path_mnist_train)\n",
    "mnist_test = pd.read_csv(local_path_mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d81f98-f812-4530-929d-237f54c277f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b191fe5-72f0-4683-aafd-4a572873a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X=torch.tensor(X,dtype=torch.float)\n",
    "y=torch.tensor(y,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a7aa24-315d-4ca2-9294-a1a2eea2a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=torch.randn((784,3),dtype=torch.float)\n",
    "b1=torch.randn((3,),dtype=torch.float)\n",
    "w2=torch.randn((3,10),dtype=torch.float)\n",
    "b2=torch.randn((10,),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ec9a6c-f0a4-48f1-b477-137bb677c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X,w1,b1,w2,b2):\n",
    "    z1=torch.matmul(X.float(),w1)+b1\n",
    "    res1=torch.sigmoid(z1)\n",
    "    z2=torch.matmul(res1,w2)+b2\n",
    "    probs=torch.softmax(z2,axis=1)\n",
    "    return probs\n",
    "def CE(probs,y):\n",
    "    return -torch.log(probs[range(y.shape[0]),y.long()]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c4a82d-8230-4063-b913-da6945863a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "p=network(X,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87ba4ce-8235-4549-960b-94d4ddc5f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9186)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loss\n",
    "CE(p,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f17e25-d7b9-42bf-8183-3caa80b8185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, loss 3.1811439990997314, acc 0.1008809506893158\n",
      "Iter 2, loss 3.1651079654693604, acc 0.09964285790920258\n",
      "Iter 3, loss 3.1496219635009766, acc 0.09883332997560501\n",
      "Iter 4, loss 3.134650230407715, acc 0.0984523817896843\n",
      "Iter 5, loss 3.1201581954956055, acc 0.09823809564113617\n",
      "Iter 6, loss 3.1061158180236816, acc 0.09811905026435852\n",
      "Iter 7, loss 3.0924930572509766, acc 0.0979047641158104\n",
      "Iter 8, loss 3.0792653560638428, acc 0.09761904925107956\n",
      "Iter 9, loss 3.066408157348633, acc 0.09702380746603012\n",
      "Iter 10, loss 3.0538992881774902, acc 0.09690476208925247\n"
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "w1=torch.randn((784,3),dtype=torch.float,requires_grad=True)\n",
    "b1=torch.randn((3,),dtype=torch.float,requires_grad=True)\n",
    "w2=torch.randn((3,10),dtype=torch.float,requires_grad=True)\n",
    "b2=torch.randn((10,),dtype=torch.float,requires_grad=True)\n",
    "lr=0.1\n",
    "Loss=[]\n",
    "for i in range(10):\n",
    "    p=network(X,w1,b1,w2,b2)\n",
    "    #print(p)\n",
    "    loss=CE(p,y)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.item())\n",
    "    acc=(p.argmax(axis=1)==y).float().mean().item()\n",
    "    print(f\"Iter {i+1}, loss {loss.item()}, acc {acc}\")\n",
    "    with torch.no_grad():\n",
    "        w1-=lr*w1.grad\n",
    "        b1-=lr*b1.grad\n",
    "        w2-=lr*w2.grad\n",
    "        b2-=lr*b2.grad\n",
    "        w1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b2.grad.zero_()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26a0299-3750-4a20-b9e0-287199681657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        X=self.X[idx,]\n",
    "        y=self.y[idx]\n",
    "        sample={'X':X,'y':y}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b60278-914b-4ba3-9828-132001559275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "mnist=MnistData(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8750258b-dc35-4968-b7b7-d0f9d9501601",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_batched=DataLoader(mnist,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965889d1-aadc-4334-905a-b8903e8b9dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " 'y': tensor([1, 0, 1, 4, 0, 0, 7, 3, 5, 3, 8, 9, 1, 3, 3, 1, 2, 0, 7, 5, 8, 6, 2, 0,\n",
       "         2, 3, 6, 9, 9, 7, 8, 9, 4, 9, 2, 1, 3, 1, 1, 4, 9, 1, 4, 4, 2, 6, 3, 7,\n",
       "         7, 4, 7, 5, 1, 9, 0, 2, 2, 3, 9, 1, 1, 1, 5, 0, 6, 3, 4, 8, 1, 0, 3, 9,\n",
       "         6, 2, 6, 4, 7, 1, 4, 1, 5, 4, 8, 9, 2, 9, 9, 8, 9, 6, 3, 6, 4, 6, 2, 9,\n",
       "         1, 2, 0, 5])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(mnist_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46195e30-4bfe-47b1-b444-bd0d1c4e7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Excercise use the regression.csv and create a Dataloader using that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd96546b-7961-49da-ab79-470be72dcf21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegData(Dataset):\n",
    "    def __init__(self,path,y_name):\n",
    "        self.path = path\n",
    "        self.y_name = y_name\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.X = self.data.drop(self.y_name,axis=1).values\n",
    "        self.y = self.data[self.y_name].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return {'X':x,'y':y}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1c14e7f-96b6-4a2a-9a2c-689d8703fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = RegData(path=\"./data/regression.csv\",y_name='mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59c4b671-4b0f-42e9-a8e8-89a6b43db53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = DataLoader(d,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccf83e3-9fd5-46c7-860b-69d9e5796d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[8.0000e+00, 3.0700e+02, 1.3000e+02, 3.5040e+03, 1.2000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.5000e+02, 1.6500e+02, 3.6930e+03, 1.1500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.1800e+02, 1.5000e+02, 3.4360e+03, 1.1000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0400e+02, 1.5000e+02, 3.4330e+03, 1.2000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0200e+02, 1.4000e+02, 3.4490e+03, 1.0500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.2900e+02, 1.9800e+02, 4.3410e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5400e+02, 2.2000e+02, 4.3540e+03, 9.0000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.4000e+02, 2.1500e+02, 4.3120e+03, 8.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5500e+02, 2.2500e+02, 4.4250e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.9000e+02, 1.9000e+02, 3.8500e+03, 8.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.8300e+02, 1.7000e+02, 3.5630e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.4000e+02, 1.6000e+02, 3.6090e+03, 8.0000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.0000e+02, 1.5000e+02, 3.7610e+03, 9.5000e+00, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 4.5500e+02, 2.2500e+02, 3.0860e+03, 1.0000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 1.1300e+02, 9.5000e+01, 2.3720e+03, 1.5000e+01, 7.0000e+01,\n",
       "          3.0000e+00],\n",
       "         [6.0000e+00, 1.9800e+02, 9.5000e+01, 2.8330e+03, 1.5500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [6.0000e+00, 1.9900e+02, 9.7000e+01, 2.7740e+03, 1.5500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [6.0000e+00, 2.0000e+02, 8.5000e+01, 2.5870e+03, 1.6000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 8.8000e+01, 2.1300e+03, 1.4500e+01, 7.0000e+01,\n",
       "          3.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 4.6000e+01, 1.8350e+03, 2.0500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.1000e+02, 8.7000e+01, 2.6720e+03, 1.7500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.0700e+02, 9.0000e+01, 2.4300e+03, 1.4500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.0400e+02, 9.5000e+01, 2.3750e+03, 1.7500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [4.0000e+00, 1.2100e+02, 1.1300e+02, 2.2340e+03, 1.2500e+01, 7.0000e+01,\n",
       "          2.0000e+00],\n",
       "         [6.0000e+00, 1.9900e+02, 9.0000e+01, 2.6480e+03, 1.5000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.6000e+02, 2.1500e+02, 4.6150e+03, 1.4000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0700e+02, 2.0000e+02, 4.3760e+03, 1.5000e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.1800e+02, 2.1000e+02, 4.3820e+03, 1.3500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [8.0000e+00, 3.0400e+02, 1.9300e+02, 4.7320e+03, 1.8500e+01, 7.0000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 9.7000e+01, 8.8000e+01, 2.1300e+03, 1.4500e+01, 7.1000e+01,\n",
       "          3.0000e+00],\n",
       "         [4.0000e+00, 1.4000e+02, 9.0000e+01, 2.2640e+03, 1.5500e+01, 7.1000e+01,\n",
       "          1.0000e+00],\n",
       "         [4.0000e+00, 1.1300e+02, 9.5000e+01, 2.2280e+03, 1.4000e+01, 7.1000e+01,\n",
       "          3.0000e+00]], dtype=torch.float64),\n",
       " 'y': tensor([18., 15., 18., 16., 17., 15., 14., 14., 14., 15., 15., 14., 15., 14.,\n",
       "         24., 22., 18., 21., 27., 26., 25., 24., 25., 26., 21., 10., 10., 11.,\n",
       "          9., 27., 28., 25.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(d_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973e4c6-6aca-425c-88e7-b9a41963a21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training using a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e959c9a-5a11-4d6a-8b32-5201cf95c62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 2.9227001667022705, acc 0.05000000074505806\n",
      "Epoch 1, iter 101, loss 2.6517159938812256, acc 0.05999999865889549\n",
      "Epoch 1, iter 201, loss 2.699867010116577, acc 0.17000000178813934\n",
      "Epoch 1, iter 301, loss 2.7328686714172363, acc 0.07000000029802322\n",
      "Epoch 1, iter 401, loss 2.6684155464172363, acc 0.029999999329447746\n",
      "Epoch 2, iter 1, loss 2.491816759109497, acc 0.07999999821186066\n",
      "Epoch 2, iter 101, loss 2.370453119277954, acc 0.10000000149011612\n",
      "Epoch 2, iter 201, loss 2.4859681129455566, acc 0.17000000178813934\n",
      "Epoch 2, iter 301, loss 2.49375581741333, acc 0.05999999865889549\n",
      "Epoch 2, iter 401, loss 2.4474682807922363, acc 0.03999999910593033\n",
      "Epoch 3, iter 1, loss 2.326808214187622, acc 0.09000000357627869\n",
      "Epoch 3, iter 101, loss 2.270258903503418, acc 0.15000000596046448\n",
      "Epoch 3, iter 201, loss 2.369637966156006, acc 0.10000000149011612\n",
      "Epoch 3, iter 301, loss 2.3730409145355225, acc 0.10000000149011612\n",
      "Epoch 3, iter 401, loss 2.3355281352996826, acc 0.14000000059604645\n",
      "Epoch 4, iter 1, loss 2.245518207550049, acc 0.12999999523162842\n",
      "Epoch 4, iter 101, loss 2.235442638397217, acc 0.11999999731779099\n",
      "Epoch 4, iter 201, loss 2.308401584625244, acc 0.09000000357627869\n",
      "Epoch 4, iter 301, loss 2.3121824264526367, acc 0.10999999940395355\n",
      "Epoch 4, iter 401, loss 2.2790815830230713, acc 0.17000000178813934\n",
      "Epoch 5, iter 1, loss 2.20161509513855, acc 0.12999999523162842\n",
      "Epoch 5, iter 101, loss 2.2237391471862793, acc 0.11999999731779099\n",
      "Epoch 5, iter 201, loss 2.2741940021514893, acc 0.09000000357627869\n",
      "Epoch 5, iter 301, loss 2.2773337364196777, acc 0.11999999731779099\n",
      "Epoch 5, iter 401, loss 2.2481236457824707, acc 0.1899999976158142\n",
      "Epoch 6, iter 1, loss 2.1725378036499023, acc 0.12999999523162842\n",
      "Epoch 6, iter 101, loss 2.217156410217285, acc 0.15000000596046448\n",
      "Epoch 6, iter 201, loss 2.2493224143981934, acc 0.10999999940395355\n",
      "Epoch 6, iter 301, loss 2.2531347274780273, acc 0.10999999940395355\n",
      "Epoch 6, iter 401, loss 2.2272653579711914, acc 0.1899999976158142\n",
      "Epoch 7, iter 1, loss 2.148231267929077, acc 0.17000000178813934\n",
      "Epoch 7, iter 101, loss 2.210036516189575, acc 0.15000000596046448\n",
      "Epoch 7, iter 201, loss 2.2268190383911133, acc 0.15000000596046448\n",
      "Epoch 7, iter 301, loss 2.2337656021118164, acc 0.1899999976158142\n",
      "Epoch 7, iter 401, loss 2.209688663482666, acc 0.18000000715255737\n",
      "Epoch 8, iter 1, loss 2.1256182193756104, acc 0.23000000417232513\n",
      "Epoch 8, iter 101, loss 2.201270341873169, acc 0.1599999964237213\n",
      "Epoch 8, iter 201, loss 2.2051737308502197, acc 0.20000000298023224\n",
      "Epoch 8, iter 301, loss 2.2167389392852783, acc 0.1899999976158142\n",
      "Epoch 8, iter 401, loss 2.192310333251953, acc 0.20000000298023224\n",
      "Epoch 9, iter 1, loss 2.1038265228271484, acc 0.23999999463558197\n",
      "Epoch 9, iter 101, loss 2.1907711029052734, acc 0.17000000178813934\n",
      "Epoch 9, iter 201, loss 2.184361696243286, acc 0.20000000298023224\n",
      "Epoch 9, iter 301, loss 2.200955629348755, acc 0.2199999988079071\n",
      "Epoch 9, iter 401, loss 2.1735472679138184, acc 0.1899999976158142\n",
      "Epoch 10, iter 1, loss 2.0822455883026123, acc 0.25\n",
      "Epoch 10, iter 101, loss 2.1784024238586426, acc 0.18000000715255737\n",
      "Epoch 10, iter 201, loss 2.16414475440979, acc 0.20999999344348907\n",
      "Epoch 10, iter 301, loss 2.1859993934631348, acc 0.20999999344348907\n",
      "Epoch 10, iter 401, loss 2.1525652408599854, acc 0.17000000178813934\n"
     ]
    }
   ],
   "source": [
    "w1=torch.randn((784,3),dtype=torch.float,requires_grad=True)\n",
    "b1=torch.randn((3,),dtype=torch.float,requires_grad=True)\n",
    "w2=torch.randn((3,10),dtype=torch.float,requires_grad=True)\n",
    "b2=torch.randn((10,),dtype=torch.float,requires_grad=True)\n",
    "lr=0.01\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=network(x,w1,b1,w2,b2)\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        with torch.no_grad():\n",
    "            w1-=lr*w1.grad\n",
    "            b1-=lr*b1.grad\n",
    "            w2-=lr*w2.grad\n",
    "            b2-=lr*b2.grad\n",
    "            w1.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "            b2.grad.zero_()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb301bfe-87e0-4d50-b399-3a2e97e1374f",
   "metadata": {},
   "source": [
    "### Writting a network with a high level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b20d81b-3bff-4213-b21d-5ff35c123f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b00fc2e3-7051-4ff7-af95-7c9c3960fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a model with nn class ####\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1=nn.Parameter(torch.randn((784,3),dtype=torch.float))\n",
    "        self.b1=nn.Parameter(torch.randn((3,),dtype=torch.float))\n",
    "        self.w2=nn.Parameter(torch.randn((3,10),dtype=torch.float))\n",
    "        self.b2=nn.Parameter(torch.randn((10,),dtype=torch.float))\n",
    "    def forward(self,X):\n",
    "        z1=torch.matmul(X.float(),self.w1)+self.b1\n",
    "        res1=torch.sigmoid(z1)\n",
    "        z2=torch.matmul(res1,self.w2)+self.b2\n",
    "        probs=torch.softmax(z2,axis=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35cc3ead-f4b5-46f7-a49b-d7f803e82e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af2b25a-8208-42fb-9ed1-dbb995df6728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 3.1765024662017822, acc 0.009999999776482582\n",
      "Epoch 1, iter 101, loss 2.939328670501709, acc 0.03999999910593033\n",
      "Epoch 1, iter 201, loss 2.653698682785034, acc 0.07999999821186066\n",
      "Epoch 1, iter 301, loss 2.6578798294067383, acc 0.03999999910593033\n",
      "Epoch 1, iter 401, loss 2.7457642555236816, acc 0.10999999940395355\n",
      "Epoch 2, iter 1, loss 2.59014892578125, acc 0.09000000357627869\n",
      "Epoch 2, iter 101, loss 2.5368409156799316, acc 0.07999999821186066\n",
      "Epoch 2, iter 201, loss 2.3850996494293213, acc 0.10999999940395355\n",
      "Epoch 2, iter 301, loss 2.4148108959198, acc 0.07999999821186066\n",
      "Epoch 2, iter 401, loss 2.4714813232421875, acc 0.14000000059604645\n",
      "Epoch 3, iter 1, loss 2.3903872966766357, acc 0.09000000357627869\n",
      "Epoch 3, iter 101, loss 2.3906188011169434, acc 0.10999999940395355\n",
      "Epoch 3, iter 201, loss 2.2657999992370605, acc 0.10999999940395355\n",
      "Epoch 3, iter 301, loss 2.3142194747924805, acc 0.10000000149011612\n",
      "Epoch 3, iter 401, loss 2.3267226219177246, acc 0.15000000596046448\n",
      "Epoch 4, iter 1, loss 2.289285182952881, acc 0.10000000149011612\n",
      "Epoch 4, iter 101, loss 2.3154959678649902, acc 0.14000000059604645\n",
      "Epoch 4, iter 201, loss 2.205848455429077, acc 0.15000000596046448\n",
      "Epoch 4, iter 301, loss 2.2676889896392822, acc 0.15000000596046448\n",
      "Epoch 4, iter 401, loss 2.248777389526367, acc 0.18000000715255737\n",
      "Epoch 5, iter 1, loss 2.229276180267334, acc 0.10000000149011612\n",
      "Epoch 5, iter 101, loss 2.2703046798706055, acc 0.17000000178813934\n",
      "Epoch 5, iter 201, loss 2.168611526489258, acc 0.18000000715255737\n",
      "Epoch 5, iter 301, loss 2.236752986907959, acc 0.15000000596046448\n",
      "Epoch 5, iter 401, loss 2.2035367488861084, acc 0.18000000715255737\n",
      "Epoch 6, iter 1, loss 2.1857824325561523, acc 0.1899999976158142\n",
      "Epoch 6, iter 101, loss 2.2349820137023926, acc 0.17000000178813934\n",
      "Epoch 6, iter 201, loss 2.1365559101104736, acc 0.20999999344348907\n",
      "Epoch 6, iter 301, loss 2.2073395252227783, acc 0.1899999976158142\n",
      "Epoch 6, iter 401, loss 2.1708850860595703, acc 0.18000000715255737\n",
      "Epoch 7, iter 1, loss 2.1477270126342773, acc 0.2199999988079071\n",
      "Epoch 7, iter 101, loss 2.199693441390991, acc 0.1899999976158142\n",
      "Epoch 7, iter 201, loss 2.1032416820526123, acc 0.23000000417232513\n",
      "Epoch 7, iter 301, loss 2.1762118339538574, acc 0.1899999976158142\n",
      "Epoch 7, iter 401, loss 2.1420252323150635, acc 0.1899999976158142\n",
      "Epoch 8, iter 1, loss 2.1108837127685547, acc 0.23000000417232513\n",
      "Epoch 8, iter 101, loss 2.160045862197876, acc 0.20000000298023224\n",
      "Epoch 8, iter 201, loss 2.066159725189209, acc 0.27000001072883606\n",
      "Epoch 8, iter 301, loss 2.1430504322052, acc 0.2199999988079071\n",
      "Epoch 8, iter 401, loss 2.1134376525878906, acc 0.2199999988079071\n",
      "Epoch 9, iter 1, loss 2.073587417602539, acc 0.23999999463558197\n",
      "Epoch 9, iter 101, loss 2.1162331104278564, acc 0.23000000417232513\n",
      "Epoch 9, iter 201, loss 2.0257468223571777, acc 0.27000001072883606\n",
      "Epoch 9, iter 301, loss 2.108947515487671, acc 0.2199999988079071\n",
      "Epoch 9, iter 401, loss 2.0839672088623047, acc 0.23000000417232513\n",
      "Epoch 10, iter 1, loss 2.035496473312378, acc 0.25\n",
      "Epoch 10, iter 101, loss 2.0730855464935303, acc 0.23999999463558197\n",
      "Epoch 10, iter 201, loss 1.9851382970809937, acc 0.36000001430511475\n",
      "Epoch 10, iter 301, loss 2.075808525085449, acc 0.3100000023841858\n",
      "Epoch 10, iter 401, loss 2.054692268371582, acc 0.25999999046325684\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x)\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        with torch.no_grad():\n",
    "            for p in mod.parameters():\n",
    "                p-=lr*p.grad\n",
    "            mod.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75e9f450-66b4-4526-8e60-ab8606d15a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can we improve this further, should we be declaring parameters? Shouldn't there be abstractions for layers?\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1=nn.Linear(784,3)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.lin2=nn.Linear(3,10)\n",
    "        self.softmax=nn.Softmax()\n",
    "    def forward(self,X):\n",
    "        x=self.lin1(X)\n",
    "        x=self.sig(x)\n",
    "        x=self.lin2(x)\n",
    "        x=self.softmax(x)\n",
    "        return x           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce771811-241c-4d6c-9a2c-fd9f9a939e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeefcb47-543f-48fc-ab3c-48cd69c08fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9dbdfe1-6089-4083-8915-0ed2f4c92758",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optim.SGD(mod.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88fb4fe6-057b-454d-9f19-81f3a0fcf2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 2.4486403465270996, acc 0.14000000059604645\n",
      "Epoch 1, iter 101, loss 2.1653308868408203, acc 0.3799999952316284\n",
      "Epoch 1, iter 201, loss 1.9942572116851807, acc 0.41999998688697815\n",
      "Epoch 1, iter 301, loss 1.8990768194198608, acc 0.38999998569488525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/ipykernel_73154/2704172253.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 401, loss 1.8355108499526978, acc 0.4000000059604645\n",
      "Epoch 2, iter 1, loss 1.7177814245224, acc 0.4099999964237213\n",
      "Epoch 2, iter 101, loss 1.6351189613342285, acc 0.3799999952316284\n",
      "Epoch 2, iter 201, loss 1.5560745000839233, acc 0.46000000834465027\n",
      "Epoch 2, iter 301, loss 1.5303338766098022, acc 0.5099999904632568\n",
      "Epoch 2, iter 401, loss 1.533867359161377, acc 0.5400000214576721\n",
      "Epoch 3, iter 1, loss 1.4266786575317383, acc 0.4699999988079071\n",
      "Epoch 3, iter 101, loss 1.3606985807418823, acc 0.49000000953674316\n",
      "Epoch 3, iter 201, loss 1.3400490283966064, acc 0.47999998927116394\n",
      "Epoch 3, iter 301, loss 1.3406490087509155, acc 0.5099999904632568\n",
      "Epoch 3, iter 401, loss 1.3999136686325073, acc 0.550000011920929\n",
      "Epoch 4, iter 1, loss 1.3073148727416992, acc 0.5099999904632568\n",
      "Epoch 4, iter 101, loss 1.2108964920043945, acc 0.5199999809265137\n",
      "Epoch 4, iter 201, loss 1.2243218421936035, acc 0.5\n",
      "Epoch 4, iter 301, loss 1.235899806022644, acc 0.5400000214576721\n",
      "Epoch 4, iter 401, loss 1.3224726915359497, acc 0.5299999713897705\n",
      "Epoch 5, iter 1, loss 1.240141749382019, acc 0.5099999904632568\n",
      "Epoch 5, iter 101, loss 1.1228001117706299, acc 0.5699999928474426\n",
      "Epoch 5, iter 201, loss 1.156173586845398, acc 0.5199999809265137\n",
      "Epoch 5, iter 301, loss 1.1700993776321411, acc 0.6000000238418579\n",
      "Epoch 5, iter 401, loss 1.2682925462722778, acc 0.5400000214576721\n",
      "Epoch 6, iter 1, loss 1.1890007257461548, acc 0.5299999713897705\n",
      "Epoch 6, iter 101, loss 1.0649549961090088, acc 0.5799999833106995\n",
      "Epoch 6, iter 201, loss 1.1084442138671875, acc 0.5699999928474426\n",
      "Epoch 6, iter 301, loss 1.120021939277649, acc 0.6499999761581421\n",
      "Epoch 6, iter 401, loss 1.2235506772994995, acc 0.5699999928474426\n",
      "Epoch 7, iter 1, loss 1.1426645517349243, acc 0.5899999737739563\n",
      "Epoch 7, iter 101, loss 1.0232161283493042, acc 0.5600000023841858\n",
      "Epoch 7, iter 201, loss 1.0688035488128662, acc 0.6000000238418579\n",
      "Epoch 7, iter 301, loss 1.0779831409454346, acc 0.6700000166893005\n",
      "Epoch 7, iter 401, loss 1.1827244758605957, acc 0.6100000143051147\n",
      "Epoch 8, iter 1, loss 1.1021310091018677, acc 0.6100000143051147\n",
      "Epoch 8, iter 101, loss 0.9912927150726318, acc 0.5699999928474426\n",
      "Epoch 8, iter 201, loss 1.0328223705291748, acc 0.6100000143051147\n",
      "Epoch 8, iter 301, loss 1.04297935962677, acc 0.6700000166893005\n",
      "Epoch 8, iter 401, loss 1.1453766822814941, acc 0.6200000047683716\n",
      "Epoch 9, iter 1, loss 1.0690808296203613, acc 0.6100000143051147\n",
      "Epoch 9, iter 101, loss 0.9662524461746216, acc 0.5899999737739563\n",
      "Epoch 9, iter 201, loss 0.9998605847358704, acc 0.6399999856948853\n",
      "Epoch 9, iter 301, loss 1.0154398679733276, acc 0.6600000262260437\n",
      "Epoch 9, iter 401, loss 1.1127028465270996, acc 0.6000000238418579\n",
      "Epoch 10, iter 1, loss 1.0419272184371948, acc 0.5899999737739563\n",
      "Epoch 10, iter 101, loss 0.9458810687065125, acc 0.6000000238418579\n",
      "Epoch 10, iter 201, loss 0.970643162727356, acc 0.6499999761581421\n",
      "Epoch 10, iter 301, loss 0.9935353994369507, acc 0.6499999761581421\n",
      "Epoch 10, iter 401, loss 1.0847772359848022, acc 0.6100000143051147\n"
     ]
    }
   ],
   "source": [
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x.float())\n",
    "        loss=CE(p,Y)\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aade743-989f-492d-a48f-d908c5f85be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can make one last change, instead of defining a loss function ourselves we will use a predifined one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d2d49d8-fde4-4f8f-8f13-4d74d552a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "030a9728-a6c5-4bb3-bbb4-521d4bef29ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 1, loss 2.3066108226776123, acc 0.12999999523162842\n",
      "Epoch 1, iter 101, loss 2.3004531860351562, acc 0.1899999976158142\n",
      "Epoch 1, iter 201, loss 2.287522792816162, acc 0.25\n",
      "Epoch 1, iter 301, loss 2.2897162437438965, acc 0.20999999344348907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/ipykernel_73154/2704172253.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iter 401, loss 2.298501968383789, acc 0.14000000059604645\n",
      "Epoch 2, iter 1, loss 2.293881416320801, acc 0.14000000059604645\n",
      "Epoch 2, iter 101, loss 2.2832610607147217, acc 0.20999999344348907\n",
      "Epoch 2, iter 201, loss 2.2658064365386963, acc 0.2800000011920929\n",
      "Epoch 2, iter 301, loss 2.2735707759857178, acc 0.17000000178813934\n",
      "Epoch 2, iter 401, loss 2.2860660552978516, acc 0.14000000059604645\n",
      "Epoch 3, iter 1, loss 2.278973340988159, acc 0.18000000715255737\n",
      "Epoch 3, iter 101, loss 2.2609593868255615, acc 0.2199999988079071\n",
      "Epoch 3, iter 201, loss 2.2315773963928223, acc 0.27000001072883606\n",
      "Epoch 3, iter 301, loss 2.252167224884033, acc 0.17000000178813934\n",
      "Epoch 3, iter 401, loss 2.270395278930664, acc 0.17000000178813934\n",
      "Epoch 4, iter 1, loss 2.258037805557251, acc 0.18000000715255737\n",
      "Epoch 4, iter 101, loss 2.232966184616089, acc 0.23000000417232513\n",
      "Epoch 4, iter 201, loss 2.1896190643310547, acc 0.3100000023841858\n",
      "Epoch 4, iter 301, loss 2.227919578552246, acc 0.20000000298023224\n",
      "Epoch 4, iter 401, loss 2.252189874649048, acc 0.20000000298023224\n",
      "Epoch 5, iter 1, loss 2.2349741458892822, acc 0.23000000417232513\n",
      "Epoch 5, iter 101, loss 2.2073700428009033, acc 0.25\n",
      "Epoch 5, iter 201, loss 2.1521615982055664, acc 0.36000001430511475\n",
      "Epoch 5, iter 301, loss 2.2028801441192627, acc 0.23999999463558197\n",
      "Epoch 5, iter 401, loss 2.228682041168213, acc 0.20999999344348907\n",
      "Epoch 6, iter 1, loss 2.212937355041504, acc 0.25999999046325684\n",
      "Epoch 6, iter 101, loss 2.184596538543701, acc 0.27000001072883606\n",
      "Epoch 6, iter 201, loss 2.1177408695220947, acc 0.3799999952316284\n",
      "Epoch 6, iter 301, loss 2.176673173904419, acc 0.28999999165534973\n",
      "Epoch 6, iter 401, loss 2.202838897705078, acc 0.27000001072883606\n",
      "Epoch 7, iter 1, loss 2.192434787750244, acc 0.3100000023841858\n",
      "Epoch 7, iter 101, loss 2.165562391281128, acc 0.33000001311302185\n",
      "Epoch 7, iter 201, loss 2.091313362121582, acc 0.4099999964237213\n",
      "Epoch 7, iter 301, loss 2.1543047428131104, acc 0.38999998569488525\n",
      "Epoch 7, iter 401, loss 2.18168568611145, acc 0.3199999928474426\n",
      "Epoch 8, iter 1, loss 2.174680709838867, acc 0.33000001311302185\n",
      "Epoch 8, iter 101, loss 2.151772975921631, acc 0.3199999928474426\n",
      "Epoch 8, iter 201, loss 2.0737531185150146, acc 0.41999998688697815\n",
      "Epoch 8, iter 301, loss 2.136232376098633, acc 0.38999998569488525\n",
      "Epoch 8, iter 401, loss 2.164614200592041, acc 0.3400000035762787\n",
      "Epoch 9, iter 1, loss 2.159520387649536, acc 0.3400000035762787\n",
      "Epoch 9, iter 101, loss 2.142103910446167, acc 0.3199999928474426\n",
      "Epoch 9, iter 201, loss 2.061516761779785, acc 0.41999998688697815\n",
      "Epoch 9, iter 301, loss 2.1215057373046875, acc 0.38999998569488525\n",
      "Epoch 9, iter 401, loss 2.150669574737549, acc 0.3400000035762787\n",
      "Epoch 10, iter 1, loss 2.146592140197754, acc 0.3499999940395355\n",
      "Epoch 10, iter 101, loss 2.1351938247680664, acc 0.3199999928474426\n",
      "Epoch 10, iter 201, loss 2.0521998405456543, acc 0.4099999964237213\n",
      "Epoch 10, iter 301, loss 2.1096904277801514, acc 0.3799999952316284\n",
      "Epoch 10, iter 401, loss 2.139467716217041, acc 0.3400000035762787\n"
     ]
    }
   ],
   "source": [
    "mod=MLP()\n",
    "opt=optim.SGD(mod.parameters(),lr=0.1)\n",
    "Loss=[]\n",
    "num_epoch=10\n",
    "for i in range(num_epoch):\n",
    "    for j,batch in enumerate(mnist_batched):\n",
    "        x=batch['X']\n",
    "        Y=batch['y']\n",
    "        p=mod(x.float())\n",
    "        loss=criterion(p,Y.long())\n",
    "        loss.backward()\n",
    "        Loss.append(loss.item())\n",
    "        acc=(p.argmax(axis=1)==Y).float().mean().item()\n",
    "        if j%100 == 0:\n",
    "            print(f\"Epoch {i+1}, iter {j+1}, loss {loss.item()}, acc {acc}\")\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d772-9494-42f1-9977-77e06c3de6f5",
   "metadata": {},
   "source": [
    "### Using keras and tensorflow to build the neural networks\n",
    "\n",
    "- Using tensors\n",
    "- Using Keras low level api\n",
    "- Using Keras Functional api\n",
    "- Using Keras Sequential api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bcc0af2-8889-4845-a36e-9b0a15a38ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7d81b-9967-4b08-af4f-42eb9dbb36b2",
   "metadata": {},
   "source": [
    "### Using tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39c3f952-399c-449c-9fc7-62390b7e3e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:03:34.406042: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-30 18:03:34.406161: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0580e968-7760-4518-a1eb-c7709ef47c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bias(k, initializer):\n",
    "    return tf.Variable(initializer(shape=[k,], dtype=tf.float32))\n",
    "def make_weights(n,k,initializer):\n",
    "    return tf.Variable(initializer(shape=[n,k], dtype=tf.float32))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3bc65a1-9d7a-47f9-a8bb-5331881a4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = make_weights(784,3,tf.random_normal_initializer())\n",
    "b1 = make_bias(3,tf.random_normal_initializer())\n",
    "w2 = make_weights(3,10,tf.random_normal_initializer())\n",
    "b2 = make_bias(10,tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af140368-13fd-44b8-9ff7-1e33319fb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X,w1,b1,w2,b2):\n",
    "    z1=tf.matmul(X,w1)+b1\n",
    "    res1=tf.math.sigmoid(z1)\n",
    "    z2=tf.matmul(res1,w2)+b2\n",
    "    probs=tf.nn.softmax(z2,axis=1)\n",
    "    return probs\n",
    "CE = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e1d8596-ddf3-4246-8397-80084aa38dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "p=network(X,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88cd9697-7307-4a26-8d18-c1294e8baae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.3035123>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE(y,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c7d2951-e7c7-4e84-ac7a-05c75e2a51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, loss 2.3025851249694824, acc 0\n",
      "Iter 2, loss 2.3025636672973633, acc 0\n",
      "Iter 3, loss 2.3025424480438232, acc 0\n",
      "Iter 4, loss 2.3025214672088623, acc 0\n",
      "Iter 5, loss 2.3025012016296387, acc 0\n",
      "Iter 6, loss 2.302480697631836, acc 0\n",
      "Iter 7, loss 2.3024609088897705, acc 0\n",
      "Iter 8, loss 2.302441120147705, acc 0\n",
      "Iter 9, loss 2.302421808242798, acc 0\n",
      "Iter 10, loss 2.3024024963378906, acc 0\n"
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')\n",
    "\n",
    "w1 = make_weights(784,3,tf.zeros_initializer())\n",
    "b1 = make_bias(3,tf.zeros_initializer())\n",
    "w2 = make_weights(3,10,tf.zeros_initializer())\n",
    "b2 = make_bias(10,tf.zeros_initializer())\n",
    "\n",
    "lr=0.1\n",
    "Loss=[]\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        p=network(X,w1,b1,w2,b2)\n",
    "        loss = CE(y,p)\n",
    "    pred = tf.cast(tf.argmax(p,axis=1),tf.int32)\n",
    "    mask = tf.equal(pred,tf.cast(y,dtype = tf.int32))\n",
    "    mask = tf.cast(mask,dtype=tf.int32)\n",
    "    acc = tf.reduce_mean(mask)\n",
    "    print(f\"Iter {i+1}, loss {loss.numpy()}, acc {acc.numpy()}\")\n",
    "    ## update the weights\n",
    "    gw1,gb1,gw2,gb2 = tape.gradient(loss,[w1,b1,w2,b2])\n",
    "    w1.assign_sub(lr*gw1)\n",
    "    b1.assign_sub(lr*gb1)\n",
    "    w2.assign_sub(lr*gw2)\n",
    "    b2.assign_sub(lr*b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa953e92-659b-47de-9ee1-6a8d49eac6d6",
   "metadata": {},
   "source": [
    "### Using keras-low-level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "390a9f5c-559b-4c26-9d74-5b27fa3fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a3cf67d-0f73-4b8d-90d3-5eb3737abb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.layers.Layer):\n",
    "    def __init__(self,layer_1_shape,layer_1_num_units, layer_2_num_units,layer_2_shape):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = Linear(units = layer_1_num_units,input_dim=layer_1_shape)\n",
    "        self.layer2 = Linear(units=layer_2_num_units,input_dim=layer_2_shape)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = tf.math.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        return tf.nn.softmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "176b0290-a1b1-4264-9205-c7ed75012554",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784,3,10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c63b9e7-af79-4a4d-ae60-7292fd3b82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist_train.drop('label',axis=1).values/255.0\n",
    "y=mnist_train['label'].values\n",
    "X = tf.constant(X,dtype='float32')\n",
    "y = tf.constant(y,dtype='float32')\n",
    "mnist_data = tf.data.Dataset.from_tensor_slices((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2df62d2f-e9a1-43c6-a3a0-9e390ce363b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist_data.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e48871a-f380-41cf-8813-c1591875ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cf7e3a0-319a-4bf2-aeb2-1f694b978cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.2941694259643555 Accuracy: 0.0\n",
      "Step: 100 Loss: 2.3048853874206543 Accuracy: 0.0\n",
      "Step: 200 Loss: 2.3071470260620117 Accuracy: 0.0\n",
      "Step: 300 Loss: 2.3042125701904297 Accuracy: 0.0\n",
      "Step: 400 Loss: 2.3023064136505127 Accuracy: 0.0\n",
      "Step: 500 Loss: 2.2969107627868652 Accuracy: 0.0\n",
      "Step: 600 Loss: 2.3000707626342773 Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y_1) in enumerate(mnist_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        probs = model(x)\n",
    "        pred = tf.cast(tf.argmax(probs,axis=1),tf.int32)\n",
    "        mask = tf.equal(pred,tf.cast(y_1,dtype=tf.int32))\n",
    "        mask = tf.cast(mask,dtype=tf.int32)\n",
    "        acc = tf.reduce_mean(mask)\n",
    "\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y_1, probs)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(model.losses)\n",
    "\n",
    "        # Get gradients of the loss wrt the weights.\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss),'Accuracy:',float(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe4b04-cb50-47ab-bbe6-5ed96e7fd63d",
   "metadata": {},
   "source": [
    "### Using keras functional api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55770adb-9044-43a8-ba25-47faf3b464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(784,))\n",
    "x=tf.keras.layers.Dense(units=3,activation=\"sigmoid\")(inputs)\n",
    "output = tf.keras.layers.Dense(units=10,activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b0df0b6-b628-4fa4-8212-f5cd4c584218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 2355      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                40        \n",
      "=================================================================\n",
      "Total params: 2,395\n",
      "Trainable params: 2,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a63da4a-4a09-4fb4-b31e-8c1d7c2760f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "700c003f-f297-4592-b0b3-d83263ee8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 25/657 [>.............................] - ETA: 2s - loss: 2.3642 - accuracy: 0.1075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:03:41.310012: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-12-30 18:03:41.310085: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 3s 4ms/step - loss: 2.2477 - accuracy: 0.1812\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.1109 - accuracy: 0.3613\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 2.0013 - accuracy: 0.4408\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 1.9027 - accuracy: 0.4427\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 1.8149 - accuracy: 0.4469\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 1.7375 - accuracy: 0.4690\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 1.6692 - accuracy: 0.4936\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 1.6089 - accuracy: 0.5139\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 1.5554 - accuracy: 0.5257\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 1.5077 - accuracy: 0.5354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ae8e2580>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_data,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97255a80-8f09-431f-980e-96c51d2911c1",
   "metadata": {},
   "source": [
    "### Keras Sequential api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "412d216d-af5d-42be-9098-14b6e811625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "    tf.keras.layers.Dense(units=3,activation='sigmoid',input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(units=10,activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6cc23e5-19a9-4571-8265-78b6dda2c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "594ca7d2-1f69-47e9-9302-eeff955825c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 28/657 [>.............................] - ETA: 2s - loss: 2.3587 - accuracy: 0.1144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:04:07.504861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 3s 4ms/step - loss: 2.2477 - accuracy: 0.1812\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 2.1110 - accuracy: 0.3611\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 2.0014 - accuracy: 0.4416\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.9028 - accuracy: 0.4433\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.8149 - accuracy: 0.4473\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.7375 - accuracy: 0.4693: 0s\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.6692 - accuracy: 0.4931\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.6089 - accuracy: 0.5132\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5554 - accuracy: 0.5258\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 1.5077 - accuracy: 0.5354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c8b99be0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_data,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c18cff-0ac7-4dc3-aecc-6e3880c9ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988d9c9-8aaa-4484-84fb-28089a8d2509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
